<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description"
    content="Fine-Tuning with Divergent Chains of Thought Boosts Reasoning Through Self-Correction in Language Models">
  <meta name="keywords" content="DCoT, Divergent Chain of Thought, CoT">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Divergent Chain of Thought (DCoT)</title>


  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <!-- Bootstrap CSS -->
  <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css">
</head>

<body>

  <nav class="navbar" role="navigation" aria-label="main navigation">
    <div class="navbar-brand">
      <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
      </a>
    </div>
    <div class="navbar-menu">
      <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
        <a class="navbar-item" href="https://haritzpuerto.github.io">
          <span class="icon">
            <i class="fas fa-home"></i>
          </span>
        </a>

        <div class="navbar-item has-dropdown is-hoverable">
          <a class="navbar-link">
            More Research
          </a>
          <div class="navbar-dropdown">
            <a class="navbar-item" href="https://code-prompting.ukp-lab.de">
              Code Prompting
            </a>
            <a class="navbar-item" href="https://square.ukp-lab.de">
              SQuARE
            </a>
          </div>
        </div>
      </div>

    </div>
  </nav>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Fine-Tuning with Divergent Chains of Thought Boosts Reasoning
              Through Self-Correction in Language Models</h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://haritzpuerto.github.io/">Haritz Puerto</a><sup>1</sup>,</span>
              <span class="author-block">
                Tilek Chubakov<sup>2</sup>,</span>
              <span class="author-block">
                <a href="https://www.xiaodanzhu.com/">Xiaodan Zhu</a><sup>3</sup>,
              </span>
              <span class="author-block">
                <a href="https://www.harishtayyarmadabushi.com/">Harish Tayyar Madabushi</a><sup>4</sup>,
              </span>
              <span class="author-block">
                <a href="https://www.informatik.tu-darmstadt.de/ukp/ukp_home/head_ukp/index.en.jsp">Iryna
                  Gurevych</a><sup>1</sup>
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>UKP Lab, Technical University of Darmstadt, Hessian.AI</span>
              <span class="author-block"><sup>2</sup>Queen's University</span>
              <span class="author-block"><sup>3</sup>University of Bath</span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- PDF Link. -->
                <span class="link-block">
                  <a href="https://arxiv.org/pdf/2011.12948" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2011.12948" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>
                <!-- Code Link. -->
                <span class="link-block">
                  <a href="https://github.com/HaritzPuerto/DCoT"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
                <!-- Dataset Link. -->
                <span class="link-block">
                  <a href="https://figshare.com/s/995e234cdfa5092749ab"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="far fa-images"></i>
                    </span>
                    <span>Data</span>
                  </a>
              </div>

            </div>
          </div>
        </div>
      </div>
    </div>
  </section>



  <section class="section">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              We present a novel method of further improving performance by requiring models to compare multiple
              reasoning chains before generating a solution in a single inference step. We call this method Divergent
              CoT (DCoT).
            </p>
            <p>
              We generate a DCoT dataset where a question is answered by a series of alternative (and correct) chains of
              thought.
              Importantly, all these CoTs are part of the same label, thus, forcing the LLM to learn how to generate
              multiple CoTs in a single inference step.
            </p>
            <p>
              We find that instruction tuning on DCoT datasets boosts the performance of LLMs of all sizes (from 1.3B to
              70B).
              These performance gains stem from models generating multiple divergent reasoning chains in a single
              inference step, indicative of the enabling of
              self-correction in language models.
            </p>
          </div>
        </div>
      </div>
      <!--/ Abstract. -->

      <!-- Intro Image -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3"></h2>
          <div class="">
            <img src="./static/images/intro.png" class="" alt="Intro" />
          </div>
        </div>
      </div>
      <!--/ Intro Image. -->
    </div>
  </section>


  <section class="section">
    <div class="container is-max-desktop">
      <!-- Method -->
      <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Diverse Chain of Thought (DCoT)</h2>
          <div class="content has-text-justified">
            <p>
              We instruction-tune LLMs to generate a sequence of divergent CoTs before selecting the final answer in a single inference step at inference time. To this end, we devise a DCoT instruction template, where we introduce a set of commands (in brackets) to request the number of CoTs to generate:
              </p>
            <!-- create centered text block -->
            <div class="content has-text-centered">
              <p>
                <b>Prompt:</b>[Question] Question [Options] Options [Number of answers] k</br>
                <b>Response:</b>[Answer 1] CoT_1 [Answer 2] ... [Answer k] CoT_k [Final answer] answer
              </p>
            </div>
          </div> 

          <h2 class="title is-3">Chain of Thought (CoT) Baseline</h2>
          <div class="content has-text-justified">
            <p>
              Similarly, to establish a comparable baseline, we instruction-tune the same LLMs using the more traditional CoT format.
              To ensure a fair comparison, we use the same reasoning chains for training.
              As shown in the figure below, each data point is composed of a question and a CoT, and a question may appear in more than one data point but with a different CoT. 
              In this way, the model leverages CoT diversity at training time but, unlike in <b>DCoT</b>, it does not do so at inference time.
            </p>
          </div> 

          <!-- Add image of the method -->
          <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
              <h2 class="title is-3"></h2>
              <div class="">
                <img src="./static/images/method.png" class="" alt="Method" />
              </div>
            </div>
          </div>

        </div>
      </div>
      <!--/ Method -->

      <!-- Data Generation -->
      <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">CoT Data Generation</h2>
          <div class="content has-text-justified">
            <p>
              We use GPT 3.5 turbo in the zero-shot setting with multiple triggers, such as <em>Let's think step by step</em> to generate CoTs.
              For each question, we select four random CoT triggers.
              We limit the number of CoTs to four to ensure that the targets fit the context window of the LLMs.
            </p>
          </div>
        </div>
      </div>
      <!--/ Data Generation -->

      <!-- Results -->
      <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Results</h2>
          <div class="content has-text-justified">
            <p>
              <table class="table table-striped table-hover">
                <thead>
                  <tr>
                    <th>Method</th>
                    <th>Phi 1.5 (1.3B)</th>
                    <th>Phi 2 (2.7B)</th>
                    <th>LLaMA 7B</th>
                    <th>LLaMA 13B</th>
                    <th>LLaMA 70B</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td><b>DCoT</b></td>
                    <td>49.39</td>
                    <td>62.60</td>
                    <td>60.80</td>
                    <td>66.18</td>
                    <td>68.63</td>
                  </tr>
                  <tr>
                    <td>CoT</td>
                    <td>47.20</td>
                    <td>60.85</td>
                    <td>58.97</td>
                    <td>64.39</td>
                    <td>66.96</td>
                  </tr>
                </tbody>
              </table>
              
            </p>

            <p>
              The table shows the average results of DCoT and CoT across 8 QA reasoning tasks.
              We observe that DCoT achieves consistent and significant performance gains compared to CoT across all LLM families and sizes.
            </p>
          </div>
        </div>
      </div>

      <!--/ Self-Correction -->
      <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Self-Correction</h2>
          <div class="content has-text-justified">
            <p>
              todo
            </p>

            <p>Here some examples will come</p>
          </div>
        </div>
      </div>



      <!-- Concurrent Work. -->
      <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Related Links</h2>

          <div class="content has-text-justified">
            <p>
              There's a lot of excellent work that was introduced around the same time as ours.
            </p>
            <p>
              <a href="https://arxiv.org/abs/2104.09125">Progressive Encoding for Neural Optimization</a> introduces an
              idea similar to our windowed position encoding for coarse-to-fine optimization.
            </p>
            <p>
              <a href="https://www.albertpumarola.com/research/D-NeRF/index.html">D-NeRF</a> and <a
                href="https://gvv.mpi-inf.mpg.de/projects/nonrigid_nerf/">NR-NeRF</a>
              both use deformation fields to model non-rigid scenes.
            </p>
            <p>
              Some works model videos with a NeRF by directly modulating the density, such as <a
                href="https://video-nerf.github.io/">Video-NeRF</a>, <a
                href="https://www.cs.cornell.edu/~zl548/NSFF/">NSFF</a>, and <a
                href="https://neural-3d-video.github.io/">DyNeRF</a>
            </p>
            <p>
              There are probably many more by the time you are reading this. Check out <a
                href="https://dellaert.github.io/NeRF/">Frank Dellart's survey on recent NeRF papers</a>, and <a
                href="https://github.com/yenchenlin/awesome-NeRF">Yen-Chen Lin's curated list of NeRF papers</a>.
            </p>
          </div>
        </div>
      </div>
      <!--/ Concurrent Work. -->

    </div>
  </section>


  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@article{park2021nerfies,
  author    = {Park, Keunhong and Sinha, Utkarsh and Barron, Jonathan T. and Bouaziz, Sofien and Goldman, Dan B and Seitz, Steven M. and Martin-Brualla, Ricardo},
  title     = {Nerfies: Deformable Neural Radiance Fields},
  journal   = {ICCV},
  year      = {2021},
}</code></pre>
    </div>
  </section>


  <footer class="footer">
    <div class="container">
      <div class="content has-text-centered">
        <a class="icon-link" href="./static/videos/nerfies_paper.pdf">
          <i class="fas fa-file-pdf"></i>
        </a>
        <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
          <i class="fab fa-github"></i>
        </a>
      </div>
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              This website is licensed under a <a rel="license"
                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
                Commons Attribution-ShareAlike 4.0 International License</a>.
            </p>
            <p>
              This means you are free to borrow the <a href="https://github.com/nerfies/nerfies.github.io">source
                code</a> of this website,
              we just ask that you link back to this page in the footer.
              Please remember to remove the analytics code included in the header of the website which
              you do not want on your website.
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>

  <script src="https://code.jquery.com/jquery-3.5.1.slim.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.5.2/dist/umd/popper.min.js"></script>
  <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js"></script>

</body>

</html>